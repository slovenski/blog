---
title: Deploying High Availability Apps with Docker Swarm
image: ../assets/swarm-glusterfs.png
date: 2020-03-28
---

![](../assets/swarm-glusterfs.png)

A good design pattern for highly available applications is to deploy the application as a container on a Docker Swarm cluster with persistent storage provided by GlusterFS. GlusterFS is a fast shared filesystem that can keep the container volume in sync between multiple VMs running the Docker Swarm cluster. This pattern ensures high availability for your containerised application. In the event a VM dies, Docker Swarm will spin up the container on another VM. GlusterFS will ensure the container has access to the same data when it comes up.

## Why GlusterFS?

After looking at several methods I settled on [GlusterFS](https://www.gluster.org/) a scalable network filesystem. Don’t get me wrong, a number of the other alternatives are pretty ground breaking and some amazing work as been put into developing them. But I don’t have thousands of dollars to drop on setting up a network file system, that may or may not work for our needs. There were also several others that I did look pretty heavily into, such as [StorageOS](https://github.com/pvdbleek/storageos) and [Ceph](https://ceph.io). With StorageOS I really liked the idea of a container based file system that stores, synchronizing, and distributes files to all other storage nodes within the swarm. And it may just be me, but Ceph looked like the prime competitor to Gluster. They both have their [high points](https://technologyadvice.com/blog/information-technology/ceph-vs-gluster) and seem to work very reliable. But at the time; it wasn’t for me and after using Gluster for a few months, I believe that I made the right choice and it’s served it’s purpose well.

And in this tutorial, we will use Docker Swarm, Google Compute Engine in the same availability zones `asia-southeast1-b` and GlusterFS to achieve our goal. And we are going to create a highly available and scalable WordPress cluster.

## Preparing The Infrastructure

First of all, we’ll need one Ubuntu 18.04 machine that will be the manager of the swarm cluster, and three Ubuntu 18.04 machine for gluster VMs that will be the worker of the swarm cluster each with 2 disks attached. We’ll use the first disk to run the OS, and the second as the GlusterFS replicated volume.

In my case, my all gluster/swarm-worker VMs had the root volume on `/dev/sda` and the second disk on `/dev/sdb` and let’s assume the private IPs and Hostnames of these VMs are below:

```
swarm-worker-1: 10.148.0.228
swarm-worker-2: 10.148.0.229
swarm-worker-3: 10.148.0.230
```

let’s check to verify the available volume on VMs:

```
# lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0     7:0    0 91.4M  1 loop /snap/core/8689
loop1     7:1    0 92.5M  1 loop /snap/google-cloud-sdk/122
sda       8:0    0   30G  0 disk
├─sda1    8:1    0 29.9G  0 part /
├─sda14   8:14   0    4M  0 part
└─sda15   8:15   0  106M  0 part /boot/efi
sdb       8:16   0   70G  0 disk
```

### Installing GlusterFS

To begin you will need to list all of the Docker Swarm worker nodes you wish to connect in the `/etc/hosts` files of each server. On linux (Debian/Ubuntu), you can get the current nodes IP Address run the following command `hostname -I | awk '{print $1}'`.

> The majority of the commands listed below need to be ran on each and every node simultaneously unless specified. To do this I opened a number of terminal tabs and connected to each server in a different tab.

```
# cat /etc/hosts
127.0.0.1    localhost
10.148.0.228 swarm-worker-1
10.148.0.229 swarm-worker-2
10.148.0.230 swarm-worker-3
```

Install GlusterFS by executing following commands:

```
# apt update && apt upgrade -y
# apt install software-properties-common -y 
# add-apt-repository ppa:gluster/glusterfs-6 && apt update 
# apt-get install glusterfs-server -y
# systemctl enable glusterd 
# systemctl start glusterd 
# systemctl status glusterd 
```

### Create Gluster "Bricks"

Now let's create the gluster data storage directories that commonly known as Brick *(**It’s very important you do this on every node.** This is because this directory is where all gluster nodes will store the distributed files locally).* Better to name them differently so it’s easy to identify on which node the replicated volumes reside. Also add an entry to your `/etc/fstab` file on each VM so that our brick gets mounted when the operating system boots or restarts.

```
[swarm-worker-1]# mkfs.xfs /dev/sdb
[swarm-worker-1]# mkdir -p /gluster/bricks/1
[swarm-worker-1]# echo '/dev/sdb /gluster/bricks/1 xfs defaults 0 0' >> /etc/fstab
[swarm-worker-1]# mount -a
[swarm-worker-1]# mkdir /gluster/bricks/1/brick

[swarm-worker-2]# mkfs.xfs /dev/sdb
[swarm-worker-2]# mkdir -p /gluster/bricks/2
[swarm-worker-2]# echo '/dev/sdb /gluster/bricks/2 xfs defaults 0 0' >> /etc/fstab
[swarm-worker-2]# mount -a
[swarm-worker-2]# mkdir /gluster/bricks/2/brick

[swarm-worker-3]# mkfs.xfs /dev/sdb
[swarm-worker-3]# mkdir -p /gluster/bricks/3
[swarm-worker-3]# echo '/dev/sdb /gluster/bricks/3 xfs defaults 0 0' >> /etc/fstab
[swarm-worker-3]# mount -a
[swarm-worker-3]# mkdir /gluster/bricks/3/brick
```

### Create Trusted Pool

Before you can configure a GlusterFS volume, you must create a trusted storage pool of the storage servers that will provide bricks to the volume by peer probing the servers. The servers in a TSP are peers of each other.

To add a server to a TSP, use command `gluster peer probe <host>` to peer probe it from one of the other servers available.

```
[swarm-worker-1]# gluster peer probe swarm-worker-2
peer probe: success.
[swarm-worker-1]# gluster peer probe swarm-worker-3
peer probe: success.
[swarm-worker-1]# gluster peer status
Number of Peers: 2

Hostname: swarm-worker-2
Uuid: e430be29-9fb1-448b-8a8c-ad9317bc5a3c
State: Peer in Cluster (Connected)

Hostname: swarm-worker-3
Uuid: 809e52a3-735e-4d41-9457-0672c2d97372
State: Peer in Cluster (Connected)
```

### Create Replicated Volume

GlusterFS has multiple [volume types](https://docs.gluster.org/en/v3/Administrator%20Guide/Setting%20Up%20Volumes/). For our HA architecture, we want to setup a “replicated” volume that stores the files created on each of the 3 VMs and hence the file is available to any app or container running on these VMs. Create the replicated volume named “gfs” with 3 replicas:

```
[swarm-worker-1]# gluster volume create gfs \
> replica 3 \
> swarm-worker-1:/gluster/bricks/1/brick \
> swarm-worker-2:/gluster/bricks/2/brick \
> swarm-worker-3:/gluster/bricks/3/brick

volume create: gfs: success: please start the volume to access data

[swarm-worker-1]# gluster volume start gfs
volume start: gfs: success

[swarm-worker-1]# gluster volume status gfs

Status of volume: gfs
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick swarm-worker-1:/gluster/bricks/1/bric
k                                           49152     0          Y       8100
Brick swarm-worker-2:/gluster/bricks/2/bric
k                                           49152     0          Y       7908
Brick swarm-worker-3:/gluster/bricks/3/bric
k                                           49152     0          Y       7917
Self-heal Daemon on localhost               N/A       N/A        Y       8121
Self-heal Daemon on swarm-worker-3          N/A       N/A        Y       7938
Self-heal Daemon on swarm-worker-2          N/A       N/A        Y       7930

Task Status of Volume gfs
------------------------------------------------------------------------------
There are no active volume tasks

[swarm-worker-1]# gluster volume info gfs

Volume Name: gfs
Type: Replicate
Volume ID: aaab7171-69f9-464e-b61d-0e086b787141
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 3 = 3
Transport-type: tcp
Bricks:
Brick1: swarm-worker-1:/gluster/bricks/1/brick
Brick2: swarm-worker-2:/gluster/bricks/2/brick
Brick3: swarm-worker-3:/gluster/bricks/3/brick
Options Reconfigured:
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off
```

### Security for the Volume

File system do not generally have firewalls as they are hosted in a secure zone of a private network. Just because it’s secure doesn’t mean you should leave it wide open for anyone with access to connect to.

Using the `auth.allow` and `auth.reject` arguments in GlusterFS we can choose which IP addresses can access the volume. Access is provided at volume level, therefore you will need to alter access permissions on every new volume you create.

To authorize all the nodes we have to connect to GlusterFS Volume, run the following command below:

```
[swarm-worker-1]# gluster volume set gfs auth.allow 10.148.0.228,10.148.0.229,10.148.0.230

```

> See the [GlusterFS Docs](https://docs.gluster.org/en/latest/Administrator%20Guide/Managing%20Volumes/) for further informations about Managing GlusterFS Volumes.

### Mount the GlusterFS

We’ll mount the volume where applications can access the files onto `/mnt` on each VM, and also append it to our `/etc/fstab` file so that it mounts on boot:

```
[swarm-worker-1]# echo 'localhost:/gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=localhost 0 0' >> /etc/fstab
[swarm-worker-1]# mount.glusterfs localhost:/gfs /mnt

[swarm-worker-2]# echo 'localhost:/gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=localhost 0 0' >> /etc/fstab
[swarm-worker-2]# mount.glusterfs localhost:/gfs /mnt

[swarm-worker-3]# echo 'localhost:/gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=localhost 0 0' >> /etc/fstab
[swarm-worker-3]# mount.glusterfs localhost:/gfs /mnt
```

and now let's verify mounted glusterfs volume:

```
# df -Th
Filesystem     Type            Size  Used Avail Use% Mounted on
udev           devtmpfs        1.8G     0  1.8G   0% /dev
tmpfs          tmpfs           369M  968K  369M   1% /run
/dev/sda1      ext4             29G  2.0G   27G   7% /
tmpfs          tmpfs           1.9G   12K  1.9G   1% /dev/shm
tmpfs          tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs          tmpfs           1.9G     0  1.9G   0% /sys/fs/cgroup
/dev/loop1     squashfs         92M   92M     0 100% /snap/core/8689
/dev/loop0     squashfs         94M   94M     0 100% /snap/core/8935
/dev/loop2     squashfs         93M   93M     0 100% /snap/google-cloud-sdk/125
/dev/sda15     vfat            105M  3.6M  101M   4% /boot/efi
/dev/sdb       xfs              70G  104M   70G   1% /gluster/bricks/1
/dev/loop4     squashfs         55M   55M     0 100% /snap/core18/1705
tmpfs          tmpfs           369M     0  369M   0% /run/user/1001
/dev/loop5     squashfs         98M   98M     0 100% /snap/google-cloud-sdk/126
localhost:/gfs fuse.glusterfs   70G  821M   70G   2% /mnt
```

The total space available on the volume comes up as 70G even though we have 3 disks of 70G each connected to GlusterFS. This is due to our replication factor of 3. Total volume size is 210G, but with a replication factor or 3 for each file only 70G is available to us.

Test GlusterFS replication:

```
[swarm-worker-1]# echo "It Works!" | tee /mnt/README.md
[swarm-worker-2]# cat /mnt/README.md
It Works!
[swarm-worker-3]# cat /mnt/README.md
It Works!
```

### Conclusion

We've managed to setup architecture for our project. The next step is to use docker stack file with our image for development which I will cover in the next part. Hasta pronto amigo!