---
title: Docker Swarm cluster with shared GlusterFS replicated volume for HA
image: ../assets/swarm-glusterfs.png
date: 2020-03-28
---

![](../assets/swarm-glusterfs.png)

A good design pattern for highly available applications is to deploy the application as a container on a Docker Swarm cluster with persistent storage provided by GlusterFS. GlusterFS is a fast shared filesystem that can keep the container volume in sync between multiple VMs running the Docker Swarm cluster. This pattern ensures high availability for your containerised application. In the event a VM dies, Docker Swarm will spin up the container on another VM. GlusterFS will ensure the container has access to the same data when it comes up.

## Why GlusterFS?

After looking at several methods I settled on [GlusterFS](https://www.gluster.org/) a scalable network filesystem. Don’t get me wrong, a number of the other alternatives are pretty ground breaking and some amazing work as been put into developing them. But I don’t have thousands of dollars to drop on setting up a network file system, that may or may not work for our needs. There were also several others that I did look pretty heavily into, such as [StorageOS](https://github.com/pvdbleek/storageos) and [Ceph](https://ceph.io). With StorageOS I really liked the idea of a container based file system that stores, synchronizing, and distributes files to all other storage nodes within the swarm. And it may just be me, but Ceph looked like the prime competitor to Gluster. They both have their [high points](https://technologyadvice.com/blog/information-technology/ceph-vs-gluster) and seem to work very reliable. But at the time; it wasn’t for me and after using Gluster for a few months, I believe that I made the right choice and it’s served it’s purpose well.

And in this tutorial, we will use Docker Swarm, Google Compute Engine in the same availability zones `asia-southeast1-b` and GlusterFS to achieve our goal. And we are going to create a highly available and scalable WordPress cluster.

## Preparing The Infrastructure

First of all, we’ll need one Ubuntu 18.04 machine that will be the manager of the swarm cluster, and three Ubuntu 18.04 machine for gluster VMs that will be the worker of the swarm cluster each with 2 disks attached. We’ll use the first disk to run the OS, and the second as the GlusterFS replicated volume.

In my case, my all gluster/swarm-worker VMs had the root volume on `/dev/sda` and the second disk on `/dev/sdb` and let’s assume the private IPs and Hostnames of these VMs are below:

```
swarm-worker-1: 10.148.0.228
swarm-worker-2: 10.148.0.229
swarm-worker-3: 10.148.0.230
```

let’s check to verify the available volume on VMs:

```
# lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0     7:0    0 91.4M  1 loop /snap/core/8689
loop1     7:1    0 92.5M  1 loop /snap/google-cloud-sdk/122
sda       8:0    0   30G  0 disk
├─sda1    8:1    0 29.9G  0 part /
├─sda14   8:14   0    4M  0 part
└─sda15   8:15   0  106M  0 part /boot/efi
sdb       8:16   0   70G  0 disk
```

### Installing GlusterFS

To begin you will need to list all of the Docker Swarm worker nodes you wish to connect in the `/etc/hosts` files of each server. On linux (Debian/Ubuntu), you can get the current nodes IP Address run the following command `hostname -I | awk '{print $1}'`.

> The majority of the commands listed below need to be ran on each and every node simultaneously unless specified. To do this I opened a number of terminal tabs and connected to each server in a different tab.

```
# cat /etc/hosts
127.0.0.1    localhost
10.148.0.228 swarm-worker-1
10.148.0.229 swarm-worker-2
10.148.0.230 swarm-worker-3
```

Install GlusterFS by executing following commands:

```
# apt update && apt upgrade -y
# apt install software-properties-common -y 
# add-apt-repository ppa:gluster/glusterfs-6 && apt update 
# apt-get install glusterfs-server -y
# systemctl enable glusterd 
# systemctl start glusterd 
# systemctl status glusterd 
```

### Create Gluster "bricks"

Now let's create the gluster data storage directories that commonly known as Brick *(**It’s very important you do this on every node.** This is because this directory is where all gluster nodes will store the distributed files locally).* Better to name them differently so it’s easy to identify on which node the replicated volumes reside. Also add an entry to your `/etc/fstab` file on each VM so that our brick gets mounted when the operating system boots or restarts.

```
[swarm-worker-1]# mkfs.xfs /dev/sdb
[swarm-worker-1]# mkdir -p /gluster/bricks/1
[swarm-worker-1]# echo '/dev/sdb /gluster/bricks/1 xfs defaults 0 0' >> /etc/fstab
[swarm-worker-1]# mount -a
[swarm-worker-1]# mkdir /gluster/bricks/1/brick

[swarm-worker-2]# mkfs.xfs /dev/sdb
[swarm-worker-2]# mkdir -p /gluster/bricks/2
[swarm-worker-2]# echo '/dev/sdb /gluster/bricks/2 xfs defaults 0 0' >> /etc/fstab
[swarm-worker-2]# mount -a
[swarm-worker-2]# mkdir /gluster/bricks/2/brick

[swarm-worker-3]# mkfs.xfs /dev/sdb
[swarm-worker-3]# mkdir -p /gluster/bricks/3
[swarm-worker-3]# echo '/dev/sdb /gluster/bricks/3 xfs defaults 0 0' >> /etc/fstab
[swarm-worker-3]# mount -a
[swarm-worker-3]# mkdir /gluster/bricks/3/brick
```

### Create trusted pool

Before you can configure a GlusterFS volume, you must create a trusted storage pool of the storage servers that will provide bricks to the volume by peer probing the servers. The servers in a TSP are peers of each other.

To add a server to a TSP, use command `gluster peer probe <host>` to peer probe it from one of the other servers available.

```
[swarm-worker-1]# gluster peer probe swarm-worker-2
peer probe: success.
[swarm-worker-1]# gluster peer probe swarm-worker-3
peer probe: success.
[swarm-worker-1]# gluster peer status
Number of Peers: 2

Hostname: swarm-worker-2
Uuid: e430be29-9fb1-448b-8a8c-ad9317bc5a3c
State: Peer in Cluster (Connected)

Hostname: swarm-worker-3
Uuid: 809e52a3-735e-4d41-9457-0672c2d97372
State: Peer in Cluster (Connected)
```

### Security for the Volume

File servers do not generally have firewalls as they are hosted in a secure zone of a private network. Just because it’s secure doesn’t mean you should leave it wide open for anyone with access to connect to.

Using the `auth.allow` and `auth.reject` arguments in GlusterFS we can choose which IP addresses can access the volume. Access is provided at volume level, therefore you will need to alter access permissions on every new volume you create.

To authorize all the nodes we have to connect to GlusterFS Volume, run the following command below:

```
[swarm-worker-1]# gluster volume set gfs auth.allow 10.148.0.228,10.148.0.229,10.148.0.230

```

> See the [GlusterFS Docs](https://docs.gluster.org/en/latest/Administrator%20Guide/Managing%20Volumes/) to further informations about Managing GlusterFS Volumes.

### 

